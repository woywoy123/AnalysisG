/**
 * @file models_implementations.dox
 * @brief Documentation for specific model implementation classes
 * @defgroup models_impl Model Implementations
 * @ingroup module_models
 * @{
 *
 * ## Overview
 *
 * Model implementations define Graph Neural Network architectures for physics analysis tasks.
 * Each implementation extends ModelTemplate and defines network layers, forward pass logic,
 * and training procedures specific to particular physics problems.
 *
 * ## Available Model Implementations
 *
 * ### GRIFT: Graph Invariant Feature Transform
 *
 * Recursive GNN architecture for jet and top quark classification.
 *
 * **Architecture Overview:**
 * - **Node encoding**: Transforms particle kinematics to latent space
 * - **Edge messaging**: Computes messages between connected particles
 * - **Recursive updates**: Iteratively refines node representations
 * - **Classification head**: Predicts particle/jet labels
 *
 * **Key Components:**
 * ```cpp
 * class grift : public model_template {
 * public:
 *     // Network parameters
 *     int _hidden = 1024;      // Hidden layer size
 *     int _xrec = 128;         // Recursive layer size
 *     int _xin = 6;            // Input features per node
 *     int _xout = 2;           // Output classes
 *     int _xtop = 5;           // Top-level features
 *     double drop_out = 0.01;  // Dropout rate
 *
 *     // Network modules
 *     torch::nn::Sequential* rnn_x;    // Node encoder
 *     torch::nn::Sequential* rnn_dx;   // Delta encoder
 *     torch::nn::Sequential* rnn_txx;  // Temporal encoder
 *     torch::nn::Sequential* rnn_rxx;  // Recursive encoder
 *     torch::nn::Sequential* rnn_hxx;  // Hidden state encoder
 *     torch::nn::Sequential* mlp_ntop; // Top prediction head
 *     torch::nn::Sequential* mlp_sig;  // Sigmoid output
 *
 *     // Control flags
 *     bool is_mc = true;       // Monte Carlo mode
 *     bool init = false;       // Initialization flag
 *     bool pagerank = false;   // Use PageRank weighting
 * };
 * ```
 *
 * **Forward Pass:**
 * ```cpp
 * void grift::forward(graph_t* g) {
 *     // 1. Encode node features
 *     torch::Tensor node_features = torch::from_blob(
 *         g->node_features.data(),
 *         {g->num_nodes, _xin},
 *         torch::kFloat32
 *     );
 *     torch::Tensor hx = node_encode(node_features, g->num_nodes, &rnn_x);
 *
 *     // 2. Message passing iterations
 *     for (int iter = 0; iter < num_iterations; ++iter) {
 *         torch::Tensor messages = torch::zeros({g->num_nodes, _hidden});
 *         
 *         // Aggregate messages from neighbors
 *         for (size_t e = 0; e < g->edge_index.size(); ++e) {
 *             int src = g->edge_index[e][0];
 *             int dst = g->edge_index[e][1];
 *             
 *             torch::Tensor msg = message(
 *                 node_features[src], node_features[dst],
 *                 g->edge_features[e],
 *                 hx[src], hx[dst]
 *             );
 *             
 *             messages[dst] += msg;
 *         }
 *         
 *         // Update hidden states
 *         hx = rnn_hxx->forward(torch::cat({hx, messages}, 1));
 *         hx = torch::dropout(hx, drop_out, this->is_training());
 *     }
 *
 *     // 3. Classification
 *     torch::Tensor logits = mlp_ntop->forward(hx);
 *     torch::Tensor probs = mlp_sig->forward(logits);
 *     
 *     g->pred = probs;
 * }
 * ```
 *
 * **Message Function:**
 * ```cpp
 * torch::Tensor grift::message(
 *     torch::Tensor trk_i,  // Source node features
 *     torch::Tensor trk_j,  // Target node features  
 *     torch::Tensor pmc,    // Edge features
 *     torch::Tensor hx_i,   // Source hidden state
 *     torch::Tensor hx_j    // Target hidden state
 * ) {
 *     // Compute relative features
 *     torch::Tensor delta = trk_j - trk_i;
 *     torch::Tensor delta_encoded = rnn_dx->forward(delta);
 *     
 *     // Concatenate all information
 *     torch::Tensor combined = torch::cat({
 *         trk_i,           // Source kinematics
 *         trk_j,           // Target kinematics
 *         delta_encoded,   // Relative features
 *         pmc,             // Edge features
 *         hx_i,            // Source state
 *         hx_j             // Target state
 *     }, 1);
 *     
 *     // Transform to message
 *     torch::Tensor message = rnn_txx->forward(combined);
 *     return message;
 * }
 * ```
 *
 * **Training Example:**
 * ```cpp
 * // Initialize model
 * grift* model = new grift();
 * model->_hidden = 1024;
 * model->_xin = 6;
 * model->_xout = 2;  // Binary classification
 * model->drop_out = 0.1;
 * model->Initialize();
 *
 * // Setup optimizer
 * torch::optim::Adam optimizer(
 *     model->parameters(),
 *     torch::optim::AdamOptions(1e-3)
 * );
 *
 * // Training loop
 * for (int epoch = 0; epoch < 100; ++epoch) {
 *     for (graph_t* batch : train_loader) {
 *         optimizer.zero_grad();
 *         
 *         model->forward(batch);
 *         torch::Tensor loss = torch::nn::functional::binary_cross_entropy(
 *             batch->pred,
 *             batch->truth
 *         );
 *         
 *         loss.backward();
 *         optimizer.step();
 *     }
 * }
 * ```
 *
 * **Inference Example:**
 * ```cpp
 * // Load trained model
 * grift* model = new grift();
 * model->Load("trained_model.pth");
 * model->eval();  // Set to evaluation mode
 *
 * // Run inference
 * for (graph_t* g : test_graphs) {
 *     model->forward(g);
 *     
 *     // Extract predictions
 *     for (int i = 0; i < g->num_nodes; ++i) {
 *         float prob = g->pred[i][1].item<float>();
 *         if (prob > 0.5) {
 *             std::cout << "Node " << i << " classified as signal" << std::endl;
 *         }
 *     }
 * }
 * ```
 *
 * ### Recursive Graph Neural Network (RecursiveGraphNeuralNetwork)
 *
 * General-purpose recursive GNN for physics object classification.
 *
 * **Architecture:**
 * - Multi-layer GNN with residual connections
 * - Attention-based edge weighting
 * - Hierarchical feature aggregation
 * - Flexible output heads (node/edge/graph-level)
 *
 * **Key Features:**
 * ```cpp
 * class RecursiveGraphNeuralNetwork : public model_template {
 * public:
 *     // Architecture parameters
 *     int num_layers = 3;
 *     int hidden_dim = 256;
 *     int num_heads = 8;        // Attention heads
 *     bool use_residual = true;
 *     bool use_batch_norm = true;
 *
 *     // Prediction targets
 *     enum TaskType {
 *         NODE_CLASSIFICATION,   // Classify individual particles
 *         EDGE_CLASSIFICATION,   // Classify particle pairs
 *         GRAPH_CLASSIFICATION   // Classify entire events
 *     };
 *     TaskType task = NODE_CLASSIFICATION;
 * };
 * ```
 *
 * **Layer Structure:**
 * ```cpp
 * class GNNLayer : public torch::nn::Module {
 * public:
 *     // Multi-head attention
 *     torch::nn::MultiheadAttention attention{nullptr};
 *     
 *     // Feed-forward network
 *     torch::nn::Sequential ffn{nullptr};
 *     
 *     // Normalization
 *     torch::nn::LayerNorm norm1{nullptr};
 *     torch::nn::LayerNorm norm2{nullptr};
 *     
 *     torch::Tensor forward(
 *         torch::Tensor x,
 *         torch::Tensor edge_index,
 *         torch::Tensor edge_attr
 *     ) {
 *         // Multi-head attention with residual
 *         torch::Tensor attn_out = attention->forward(x, x, x)[0];
 *         x = norm1->forward(x + attn_out);
 *         
 *         // Feed-forward with residual
 *         torch::Tensor ffn_out = ffn->forward(x);
 *         x = norm2->forward(x + ffn_out);
 *         
 *         return x;
 *     }
 * };
 * ```
 *
 * **Graph-Level Prediction:**
 * ```cpp
 * void RecursiveGraphNeuralNetwork::forward(graph_t* g) {
 *     torch::Tensor x = EncodeNodes(g->node_features);
 *     
 *     // Apply GNN layers
 *     for (int layer = 0; layer < num_layers; ++layer) {
 *         x = gnn_layers[layer]->forward(x, g->edge_index, g->edge_features);
 *     }
 *     
 *     if (task == GRAPH_CLASSIFICATION) {
 *         // Global pooling
 *         torch::Tensor graph_repr = torch::mean(x, 0);  // Mean pooling
 *         
 *         // Classification head
 *         torch::Tensor logits = graph_classifier->forward(graph_repr);
 *         g->pred = torch::softmax(logits, -1);
 *     }
 *     else if (task == NODE_CLASSIFICATION) {
 *         // Node-level predictions
 *         torch::Tensor logits = node_classifier->forward(x);
 *         g->pred = torch::softmax(logits, -1);
 *     }
 * }
 * ```
 *
 * ## Implementing Custom Models
 *
 * ### Step 1: Define Model Class
 *
 * ```cpp
 * #include <templates/model_template.h>
 *
 * class my_model : public model_template {
 * public:
 *     my_model();
 *     ~my_model() override;
 *
 *     // Required overrides
 *     model_template* clone() override;
 *     void forward(graph_t* g) override;
 *
 *     // Network parameters
 *     int input_dim = 6;
 *     int hidden_dim = 128;
 *     int output_dim = 2;
 *
 *     // Network modules
 *     torch::nn::Sequential encoder{nullptr};
 *     torch::nn::Sequential decoder{nullptr};
 * };
 * ```
 *
 * ### Step 2: Initialize Network
 *
 * ```cpp
 * my_model::my_model() {
 *     // Encoder
 *     encoder = torch::nn::Sequential(
 *         torch::nn::Linear(input_dim, hidden_dim),
 *         torch::nn::ReLU(),
 *         torch::nn::Dropout(0.1),
 *         torch::nn::Linear(hidden_dim, hidden_dim),
 *         torch::nn::ReLU()
 *     );
 *
 *     // Decoder
 *     decoder = torch::nn::Sequential(
 *         torch::nn::Linear(hidden_dim, output_dim),
 *         torch::nn::Softmax(1)
 *     );
 *
 *     // Register modules
 *     register_module("encoder", encoder);
 *     register_module("decoder", decoder);
 * }
 * ```
 *
 * ### Step 3: Implement Forward Pass
 *
 * ```cpp
 * void my_model::forward(graph_t* g) {
 *     // Convert to tensors
 *     torch::Tensor x = torch::from_blob(
 *         g->node_features.data(),
 *         {g->num_nodes, input_dim},
 *         torch::kFloat32
 *     );
 *
 *     // Encode
 *     torch::Tensor hidden = encoder->forward(x);
 *
 *     // Decode
 *     torch::Tensor output = decoder->forward(hidden);
 *
 *     // Store predictions
 *     g->pred = output;
 * }
 * ```
 *
 * ### Step 4: Training Setup
 *
 * ```cpp
 * // Create model
 * my_model* model = new my_model();
 * model->to(torch::kCUDA);  // Move to GPU
 *
 * // Setup training
 * Optimizer* opt = new Optimizer();
 * opt->Model = model;
 * opt->Folds = 5;
 * opt->Epochs = 100;
 * opt->BatchSize = 32;
 * opt->LearningRate = 0.001;
 *
 * // Train
 * DataLoader* loader = new DataLoader();
 * loader->SetGraphs(train_graphs);
 * loader->MakeKFolds(5);
 *
 * opt->RunTraining(loader);
 * ```
 *
 * ## Best Practices
 *
 * ### Architecture Design
 * - Start with simple baselines (MLP, shallow GNN)
 * - Add complexity incrementally (residual, attention)
 * - Monitor training/validation curves for overfitting
 *
 * ### Hyperparameter Tuning
 * - Use k-fold cross-validation
 * - Grid search or Bayesian optimization
 * - Track all experiments (wandb, tensorboard)
 *
 * ### Performance Optimization
 * - Batch graphs for GPU efficiency
 * - Use mixed precision training (torch.amp)
 * - Profile bottlenecks with torch.profiler
 * - Consider model parallelism for large graphs
 *
 * ### Regularization
 * - Dropout in hidden layers
 * - Weight decay (L2 regularization)
 * - Early stopping based on validation loss
 * - Data augmentation (graph permutations)
 *
 * ### Model Saving/Loading
 * ```cpp
 * // Save model
 * model->Save("my_model.pth");
 *
 * // Load model
 * my_model* loaded_model = new my_model();
 * loaded_model->Load("my_model.pth");
 * loaded_model->eval();
 * ```
 *
 * ## See Also
 * - @ref ModelTemplate - Base class documentation
 * - @ref Optimizer - Training orchestration
 * - @ref DataLoader - Graph batching
 * - @ref module_pyc_graph - Graph operations
 *
 * @}
 */
